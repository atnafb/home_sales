{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KW73O2e3dw",
        "outputId": "83fb814c-bb64-4491-9e1b-8c6229fdb101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [1 InRelease 3,632 B/3,\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [Waiting for headers] [\r                                                                                                    \rGet:2 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,790 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 11.7 MB in 2s (7,693 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.5.5'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "2XbWNf1Te5fM"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wOJqxG_RPSwp"
      },
      "outputs": [],
      "source": [
        "# 1. Read the home_sales_revised.csv from the provided AWS S3 bucket location into a PySpark DataFrame.\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RoljcJ7WPpnm"
      },
      "outputs": [],
      "source": [
        "# 2. Create a temporary view of the DataFrame.\n",
        "spark.sparkContext.addFile(url)\n",
        "df = spark.read.csv(SparkFiles.get(\"home_sales_revised.csv\"), header=True, inferSchema=True)\n",
        "df.createOrReplaceTempView(\"home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display the df\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9tXu_49E713",
        "outputId": "8182ffc6-9c85-493c-95e1-5792570f9b8c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|id                                  |date      |date_built|price |bedrooms|bathrooms|sqft_living|sqft_lot|floors|waterfront|view|\n",
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "|f8a53099-ba1c-47d6-9c31-7398aa8f6089|2022-04-08|2016      |936923|4       |3        |3167       |11733   |2     |1         |76  |\n",
            "|7530a2d8-1ae3-4517-9f4a-befe060c4353|2021-06-13|2013      |379628|2       |2        |2235       |14384   |1     |0         |23  |\n",
            "|43de979c-0bf0-4c9f-85ef-96dc27b258d5|2019-04-12|2014      |417866|2       |2        |2127       |10575   |2     |0         |0   |\n",
            "|b672c137-b88c-48bf-9f18-d0a4ac62fb8b|2019-10-16|2016      |239895|2       |2        |1631       |11149   |2     |0         |0   |\n",
            "|e0726d4d-d595-4074-8283-4139a54d0d63|2022-01-08|2017      |424418|3       |2        |2249       |13878   |2     |0         |4   |\n",
            "|5aa00529-0533-46ba-870c-9e881580ef35|2019-01-30|2017      |218712|2       |3        |1965       |14375   |2     |0         |7   |\n",
            "|131492a1-72e2-4a84-bf97-0db14973bfdb|2020-02-08|2017      |419199|2       |3        |2062       |8876    |2     |0         |6   |\n",
            "|8d54a71b-c520-44e5-8ba1-5a84be03ad35|2019-07-21|2010      |323956|2       |3        |1506       |11816   |1     |0         |25  |\n",
            "|e81aacfe-17fe-46b1-a52a-4753d1622b4a|2020-06-16|2016      |181925|3       |3        |2137       |11709   |2     |0         |22  |\n",
            "|2ed8d509-7372-46d5-a9dd-9281a95467d4|2021-08-06|2015      |258710|3       |3        |1918       |9666    |1     |0         |25  |\n",
            "|f876d86f-3c9f-42b6-928f-bcb685c070ed|2019-02-27|2011      |167864|3       |3        |2471       |13924   |2     |0         |15  |\n",
            "|0a2bd445-8508-4d83-b6be-f043bf3f3b24|2021-12-30|2014      |337527|2       |3        |1926       |12556   |1     |0         |23  |\n",
            "|941bad30-eb49-4a78-b83a-87abb87a62db|2020-05-09|2015      |229896|3       |3        |2197       |8641    |1     |0         |3   |\n",
            "|dd61eb34-6589-4c04-92ba-fc3c44f1b670|2021-07-25|2016      |210247|3       |2        |1672       |11986   |2     |0         |28  |\n",
            "|f1e4cef7-d151-4391-8d00-d92ed2b7ab1b|2019-02-01|2011      |398667|2       |3        |2331       |11356   |1     |0         |7   |\n",
            "|ea620c7b-c2f7-4c6f-9d16-3ccb34320ca4|2021-05-31|2011      |437958|3       |3        |2356       |11052   |1     |0         |26  |\n",
            "|f233cb41-6f33-4b0c-b9ed-1b8aa87163e5|2021-07-18|2016      |437375|4       |3        |1704       |11721   |2     |0         |34  |\n",
            "|c797ca12-52cd-4b13-9338-183653619b11|2019-06-08|2015      |288650|2       |3        |2100       |10419   |2     |0         |7   |\n",
            "|0cfe57f3-28c2-472c-9bc3-aaeac6807e62|2019-10-04|2015      |308313|3       |3        |1960       |9453    |2     |0         |2   |\n",
            "|4566cd2a-ac6e-4358-b1ad-56921991ff60|2019-07-15|2016      |177541|3       |3        |2130       |10517   |2     |0         |25  |\n",
            "+------------------------------------+----------+----------+------+--------+---------+-----------+--------+------+----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "L6fkwOeOmqvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9290f4a-b3b1-485d-a4b2-357ac1a3061c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2019| 300263.7|\n",
            "|2020|298353.78|\n",
            "|2021|301819.44|\n",
            "|2022|296363.88|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. What is the average price for a four bedroom house sold per year, rounded to two decimal places?\n",
        "query = \"\"\"\n",
        "SELECT YEAR(date) AS year, ROUND(AVG(price), 2) as avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 4\n",
        "GROUP BY year\n",
        "ORDER BY year\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "l8p_tUS8h8it",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31fc2d6-7994-45dd-ae24-262a9773c427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2010|292859.62|\n",
            "|2011|291117.47|\n",
            "|2012|293683.19|\n",
            "|2013|295962.27|\n",
            "|2014|290852.27|\n",
            "|2015| 288770.3|\n",
            "|2016|290555.07|\n",
            "|2017|292676.79|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. What is the average price of a home for each year the home was built,\n",
        "# that have 3 bedrooms and 3 bathrooms, rounded to two decimal places?\n",
        "query = \"\"\"\n",
        "SELECT date_built AS year, ROUND (AVG(price), 2) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3\n",
        "GROUP BY date_built\n",
        "ORDER BY date_built\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Y-Eytz64liDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750a190f-7d39-4b0d-c553-4a0a91d81ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|year|avg_price|\n",
            "+----+---------+\n",
            "|2010|285010.22|\n",
            "|2011|276553.81|\n",
            "|2012|307539.97|\n",
            "|2013|303676.79|\n",
            "|2014|298264.72|\n",
            "|2015|297609.97|\n",
            "|2016| 293965.1|\n",
            "|2017|280317.58|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 5. What is the average price of a home for each year the home was built,\n",
        "# that have 3 bedrooms, 3 bathrooms, with two floors,\n",
        "# and are greater than or equal to 2,000 square feet, rounded to two decimal places?\n",
        "query = \"\"\"\n",
        "SELECT date_built AS year, ROUND(AVG(price), 2) AS avg_price\n",
        "FROM home_sales\n",
        "WHERE bedrooms = 3 AND bathrooms = 3 AND floors = 2 AND sqft_living >= 2000\n",
        "GROUP BY year\n",
        "ORDER BY year\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUrfgOX1pCRd",
        "outputId": "1be58e44-ad83-408c-9159-4d8740228f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|view_rating| avg_price|\n",
            "+-----------+----------+\n",
            "|        100| 1026669.5|\n",
            "|         99|1061201.42|\n",
            "|         98|1053739.33|\n",
            "|         97|1129040.15|\n",
            "|         96|1017815.92|\n",
            "|         95| 1054325.6|\n",
            "|         94| 1033536.2|\n",
            "|         93|1026006.06|\n",
            "|         92| 970402.55|\n",
            "|         91|1137372.73|\n",
            "|         90|1062654.16|\n",
            "|         89|1107839.15|\n",
            "|         88|1031719.35|\n",
            "|         87| 1072285.2|\n",
            "|         86|1070444.25|\n",
            "|         85|1056336.74|\n",
            "|         84|1117233.13|\n",
            "|         83|1033965.93|\n",
            "|         82| 1063498.0|\n",
            "|         81|1053472.79|\n",
            "+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.46152496337890625 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 6. What is the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000? Order by descending view rating.\n",
        "# Although this is a small dataset, determine the run time for this query.\n",
        "\n",
        "start_time = time.time()\n",
        "query = \"\"\"\n",
        "SELECT view AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
        "FROM home_sales\n",
        "GROUP BY view_rating\n",
        "HAVING avg_price >= 350000\n",
        "ORDER BY view_rating DESC\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "KAhk3ZD2tFy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442a3cb8-dc53-4521-fb4a-7c9aa80d4306"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# 7. Cache the the temporary table home_sales.\n",
        "spark.sql(\"CACHE TABLE home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4opVhbvxtL-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d5650f-a7bb-4e44-c356-40bb52ebbe83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# 8. Check if the table is cached.\n",
        "spark.catalog.isCached('home_sales')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GnL46lwTSEk",
        "outputId": "ffe9059f-df2a-406e-f223-f7c4785f16d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|view_rating| avg_price|\n",
            "+-----------+----------+\n",
            "|        100| 1026669.5|\n",
            "|         99|1061201.42|\n",
            "|         98|1053739.33|\n",
            "|         97|1129040.15|\n",
            "|         96|1017815.92|\n",
            "|         95| 1054325.6|\n",
            "|         94| 1033536.2|\n",
            "|         93|1026006.06|\n",
            "|         92| 970402.55|\n",
            "|         91|1137372.73|\n",
            "|         90|1062654.16|\n",
            "|         89|1107839.15|\n",
            "|         88|1031719.35|\n",
            "|         87| 1072285.2|\n",
            "|         86|1070444.25|\n",
            "|         85|1056336.74|\n",
            "|         84|1117233.13|\n",
            "|         83|1033965.93|\n",
            "|         82| 1063498.0|\n",
            "|         81|1053472.79|\n",
            "+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.2895655632019043 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 9. Using the cached data, run the last query above, that calculates\n",
        "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000.\n",
        "# Determine the runtime and compare it to the uncached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "query = \"\"\"\n",
        "SELECT view AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
        "FROM home_sales\n",
        "GROUP BY view_rating\n",
        "HAVING avg_price >= 350000\n",
        "ORDER BY view_rating DESC\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Analysis*: The query execution time dropped from 0.46 seconds to just 0.28 seconds after caching, which is a massive improvement in speed. This confirms that Spark is now reading from memory (RAM) instead of repeatedly scanning the dataset from disk."
      ],
      "metadata": {
        "id": "gMN1UP60MKwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Qm12WN9isHBR"
      },
      "outputs": [],
      "source": [
        "# 10. Partition by the \"date_built\" field on the formatted parquet home sales data\n",
        "df.write.mode('overwrite').partitionBy('date_built').parquet('p_home_sales')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data has been saved in Parquet format. This has been confirmed manually by navigating to the file directory and executing !ls p_home_sales from the notebook. The files inside each folder contain the Parquet data corresponding to the rows with that particular date_built value. In this case, the folders are named based on the following date_built values: 2010, 2011, 2012, 2013, 2014, 2015, 2016, and 2017. If we want to query the data later, we can efficiently filter by the date_built column, and Spark will only load the relevant partitions."
      ],
      "metadata": {
        "id": "XutUHgLujv8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "AZ7BgY61sRqY"
      },
      "outputs": [],
      "source": [
        "# 11. Read the parquet formatted data.\n",
        "df_parquet = spark.read.parquet(\"p_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "J6MJkHfvVcvh"
      },
      "outputs": [],
      "source": [
        "# 12. Create a temporary table for the parquet data.\n",
        "df_parquet.createOrReplaceTempView(\"p_home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_Vhb52rU1Sn",
        "outputId": "6a48412a-87eb-43c8-d309-236769678648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+\n",
            "|view_rating| avg_price|\n",
            "+-----------+----------+\n",
            "|        100| 1026669.5|\n",
            "|         99|1061201.42|\n",
            "|         98|1053739.33|\n",
            "|         97|1129040.15|\n",
            "|         96|1017815.92|\n",
            "|         95| 1054325.6|\n",
            "|         94| 1033536.2|\n",
            "|         93|1026006.06|\n",
            "|         92| 970402.55|\n",
            "|         91|1137372.73|\n",
            "|         90|1062654.16|\n",
            "|         89|1107839.15|\n",
            "|         88|1031719.35|\n",
            "|         87| 1072285.2|\n",
            "|         86|1070444.25|\n",
            "|         85|1056336.74|\n",
            "|         84|1117233.13|\n",
            "|         83|1033965.93|\n",
            "|         82| 1063498.0|\n",
            "|         81|1053472.79|\n",
            "+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "--- 0.8122079372406006 seconds ---\n"
          ]
        }
      ],
      "source": [
        "# 13. Using the parquet DataFrame, run the last query above, that calculates\n",
        "# the average price of a home per \"view\" rating, rounded to two decimal places,\n",
        "# having an average home price greater than or equal to $350,000.\n",
        "# Determine the runtime and compare it to the cached runtime.\n",
        "\n",
        "start_time = time.time()\n",
        "query = \"\"\"\n",
        "SELECT view AS view_rating, ROUND(AVG(price), 2) AS avg_price\n",
        "FROM p_home_sales\n",
        "GROUP BY view_rating\n",
        "HAVING avg_price >= 350000\n",
        "ORDER BY view_rating DESC\n",
        "\"\"\"\n",
        "spark.sql(query).show()\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "hjjYzQGjtbq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3fd30e-df27-49a8-8917-983c0195baa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# 14. Uncache the home_sales temporary table.\n",
        "spark.sql(\"UNCACHE TABLE home_sales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Sy9NBvO7tlmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d69cfff-11f5-43c0-b1e6-76553266d774"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# 15. Check if the home_sales is no longer cached\n",
        "spark.catalog.isCached(\"home_sales\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis on query runtime:**\n",
        "\n",
        "\n",
        "* The first run without caching or partitioning took approximately 0.46 seconds.\n",
        "* After caching the data using cache table home_sales, the run time decreased to 0.28 seconds\n",
        "* After partitioning the data by date_built and creating a temporary view (p_home_sales), the run time increased to 0.89 seconds.\n",
        "\n",
        "Reason: While partitioning is a powerful optimization for large datasets, it may not always lead to an immediate reduction in query time if the dataset is small or the query does not filter by partitioned columns.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aIlxvpXntbzP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si-BNruRUGK3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}